{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81674db5",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 크롤러"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba65a1",
   "metadata": {},
   "source": [
    "### 본 크롤러는 네이버 뉴스 크롤러 입니다.\n",
    "\n",
    "## ========== ver 0.4 =========\n",
    "\n",
    "#### - <u>세부 파라미터값이 있습니다. 함수에서 직접 조절해주세요.</u>\n",
    "\n",
    "## ========== ver 0.3 =========\n",
    "\n",
    "#### - naver.news 개편에 따라 크롤링 태그 변경이 있었습니다. (update by. daesun97)\n",
    "#### - 전처리 기능을 더욱 강화했습니다. (update by. daesun97)\n",
    "#### - 기사 카테고리 분류가 추가되었습니다.\n",
    "#### - 기간내 특정 일자들의 기사를 놓치는 문제를 해결하고자 알고리즘 수정이 있었습니다.\n",
    "#### - 자동으로 검색 일자를 변경해 최소 1주일 단위로 기사를 수집합니다.\n",
    "\n",
    "## ========== ver 0.2 =========\n",
    "\n",
    "#### - 본문의 전처리가 추가되었습니다. (update by. daesun97)\n",
    "\n",
    "## ========== ver 0.1 =========\n",
    "#### - 본 크롤러는 안내에 맞게 날짜를 입력해주셔야 합니다.\n",
    "#### - 처음 사용하시는 분이라면 시작 전 본인의 컴퓨터에 !pip install fake_useragent 를 꼭 해주세요.\n",
    "#### - fake_useragent로 ip차단걱정을 덜었습니다.\n",
    "#### - 가급적 로컬 환경에서 사용해주세요. (colab 환경에서 실험을 아직 안해봤습니다)\n",
    "#### - excel로 자동 저장됩니다.\n",
    "#### - 각종 이슈 발생시 꼭 알려주세요.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### (제작자 : suhykii)\n",
    "###### 참조 블로그 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce04d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake_useragent\n",
      "  Using cached fake-useragent-0.1.11.tar.gz (13 kB)\n",
      "Building wheels for collected packages: fake-useragent\n",
      "  Building wheel for fake-useragent (setup.py): started\n",
      "  Building wheel for fake-useragent (setup.py): finished with status 'done'\n",
      "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=69bf3d1f29f7c36f6f258f1d049fbba3339878220979c0884413be8ba2ee3ffe\n",
      "  Stored in directory: c:\\users\\zls99\\appdata\\local\\pip\\cache\\wheels\\ed\\f7\\62\\50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
      "Successfully built fake-useragent\n",
      "Installing collected packages: fake-useragent\n",
      "Successfully installed fake-useragent-0.1.11\n"
     ]
    }
   ],
   "source": [
    "#!pip install fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6db515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7024ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 크롤링 함수\n",
    "def crawler(keyword, start_day, last_day):\n",
    "    df = pd.DataFrame(columns=['title', 'category', 'date', 'paper', 'url', 'content'])\n",
    "\n",
    "    last_day = pd.to_datetime(last_day)\n",
    "    front_day = pd.to_datetime(start_day)\n",
    "    back_day = pd.to_datetime(start_day) + timedelta(days=6)\n",
    "    new_len=0\n",
    "    while (last_day >= back_day) & (last_day >= front_day):\n",
    "        front_day = str(front_day)[:10]\n",
    "        back_day = str(back_day)[:10]\n",
    "        s_d_f = front_day.replace(\"-\",\".\")\n",
    "        e_d_f = back_day.replace(\"-\",\".\")\n",
    "        s_d_b = front_day.replace(\"-\",\"\")\n",
    "        e_d_b = back_day.replace(\"-\",\"\")\n",
    "        page=1\n",
    "        less_news_cnt = 0\n",
    "        old_len = new_len\n",
    "        while less_news_cnt < 8: #이 숫자가 커질수록 기사를 많이 긁어와요. 조절을 잘 해야해요.\n",
    "            url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={keyword}&sort=0&photo=0&field=0&pd=3&ds={s_d_f}&de={e_d_f}&cluster_rank=47&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from{s_d_b}to{e_d_b},a:all&start=\"+str(page)\n",
    "\n",
    "            response = requests.get(url)\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            for urls in soup.select(\"a.info\"):\n",
    "                naver_news_cnt = len([urls for urls in soup.select(\"a.info\") if urls[\"href\"].startswith(\"https://news.naver.com\") ])\n",
    "                if naver_news_cnt < 4: # 이 숫자가 작을수록 기사를 많이 긁어와요. 조절을 잘해야해요.\n",
    "                    less_news_cnt+=1\n",
    "\n",
    "                try:\n",
    "                    if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                        news_detail = []\n",
    "\n",
    "                        ua=UserAgent()\n",
    "                        headers = {\"User-Agent\" : ua.random}\n",
    "\n",
    "                        breq = requests.get(urls[\"href\"], headers = headers)\n",
    "                        bsoup = BeautifulSoup(breq.content, 'html.parser')\n",
    "\n",
    "                        title = bsoup.select('h2.media_end_head_headline')[0].text\n",
    "                        news_detail.append(title)\n",
    "\n",
    "                        category = bsoup.select('.media_end_categorize_item')[0].text\n",
    "                        news_detail.append(category)\n",
    "\n",
    "                        date = bsoup.select('._ARTICLE_DATE_TIME')[0].text[:10]\n",
    "                        date = date.replace(\".\",\"\")\n",
    "                        news_detail.append(date)\n",
    "\n",
    "                        pcompany = bsoup.select('.media_end_linked_more_point')[0].text\n",
    "                        news_detail.append(pcompany)\n",
    "\n",
    "                        news_detail.append(urls[\"href\"])\n",
    "\n",
    "                        _text = bsoup.select('#newsct_article')[0].get_text().replace('\\n', \"\")\n",
    "                        btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "                        btext=btext[:btext.find(('▶'))]\n",
    "                        btext=btext[:btext.find(('▷'))]\n",
    "                        btext=re.sub('[【<\\[].*?[>\\]】]', '', btext)\n",
    "                        btext=re.sub('[◆©ⓒ■◇]', '', btext)\n",
    "                        btext=re.sub('[a-zA-Z0-9+-_.]+@[a-zA-Z0-9]+\\.[a-zA-Z0-9-.]+\\.*[a-z]*','',btext)\n",
    "                        btext=re.sub('[0-9]+\\.[0-9]*\\.[0-9]*','',btext)\n",
    "                        btext=re.sub('동+영+상','',btext)\n",
    "                        btext=re.sub('[뉴]+[스]','',btext)\n",
    "                        btext=re.sub('사+진+\\=','',btext)\n",
    "                        btext=re.sub('\\(+사+진+=+.*?\\)','',btext)\n",
    "                        btext=re.sub('\\(+[서]+[울]+=+[연]+[합]+\\)','',btext)\n",
    "                        btext=re.sub('....[기]+[자]+.=','',btext)\n",
    "                        btext=re.sub('...[기]+[자]','',btext)\n",
    "                        btext=re.sub('연+합+뉴+스','',btext)\n",
    "                        btext=re.sub('\\(+\\)+네+이+버+.*','',btext)\n",
    "                        btext=re.sub('.+제+공+\\=','',btext)\n",
    "\n",
    "                        news_detail.append(btext.strip())\n",
    "\n",
    "                        df.loc[len(df)] = news_detail\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "\n",
    "            page += 10\n",
    "        new_len = len(df)\n",
    "        print(f\"{front_day}부터 {back_day}까지 기사를\",new_len-old_len,\"개 추가했습니다.\")\n",
    "        front_day = pd.to_datetime(front_day) + timedelta(days=7)\n",
    "        back_day = pd.to_datetime(back_day) + timedelta(days=7)\n",
    "        if back_day >= last_day:\n",
    "            back_day = last_day\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    info_main = input(\"=\"*50+\"\\n\"+\"시작전에 상단에 설명을 잘 읽어주세요\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
    "    keyword = input(\"=\"*50+\"\\n\"+\"검색하실 키워드는 어떻게 되나요?\"+\"\\n\"+\"정확하게 작성해주세요\"+\"\\n\"+\"=\"*50)\n",
    "    start_day = input(\"=\"*50+\"\\n\"+\"언제부터 시작할까요?\"+\"\\n\"+\"반드시 0000-00-00 형식으로 작성해주세요\"+\"\\n\"+\"=\"*50)\n",
    "    last_day = input(\"=\"*50+\"\\n\"+\"언제까지 탐색할까요?\"+\"\\n\"+\"반드시 0000-00-00 형식으로 작성해주세요\"+\"\\n\"+\"=\"*50)\n",
    "    print(\"=\"*40+\"크롤링을 시작합니다\"+\"=\"*40)\n",
    "    #rate = int(input(\"=\"*50+\"\\n\"+\"대략 몇개의 기사가 출력되길 원하시나요?\"+\"\\n\"+\"10의 자리수로 끊어서 숫자만\"+\"\\n\"+\"최대 5000\"+\"\\n\"+\"=\"*50))//10\n",
    "    s_d_b = start_day.replace(\"-\",\"\")[2:]\n",
    "    e_d_b = last_day.replace(\"-\",\"\")[2:]\n",
    "    df = crawler(keyword, start_day, last_day)\n",
    "    \n",
    "    df.to_excel(f'{keyword}_{s_d_b}~{e_d_b} 테스트 데이터.xlsx',index=False,encoding='utf-8-sig')\n",
    "    print(\"\\n\")\n",
    "    print(f\"현재 폴더에 {keyword}_{s_d_b}~{e_d_b} 테스트 데이터.xlsx 이름으로 저장했습니다.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e6635",
   "metadata": {},
   "source": [
    "### 종목명, 기사 기간,  기사 최대 개수를 적어주세요. \n",
    "#### 완료되면 자동으로 excel을 생성합니다. (노트 위치에 저장)\n",
    "#### 입력하신 종목, 날짜를 기준으로 파일명이 저장됩니다.\n",
    "#### \"df\" 변수로 저장되어있어 바로 확인 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3c58e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "시작전에 상단에 설명을 잘 읽어주세요\n",
      " 시작하시려면 Enter를 눌러주세요.\n",
      "==================================================\n",
      "==================================================\n",
      "검색하실 키워드는 어떻게 되나요?\n",
      "정확하게 작성해주세요\n",
      "==================================================네이버\n",
      "==================================================\n",
      "언제부터 시작할까요?\n",
      "반드시 0000-00-00 형식으로 작성해주세요\n",
      "==================================================2017-01-01\n",
      "==================================================\n",
      "언제까지 탐색할까요?\n",
      "반드시 0000-00-00 형식으로 작성해주세요\n",
      "==================================================2022-03-31\n",
      "========================================크롤링을 시작합니다========================================\n",
      "2017-01-01부터 2017-01-07까지 기사를 41 개 추가했습니다.\n",
      "2017-01-08부터 2017-01-14까지 기사를 33 개 추가했습니다.\n",
      "2017-01-15부터 2017-01-21까지 기사를 28 개 추가했습니다.\n",
      "2017-01-22부터 2017-01-28까지 기사를 153 개 추가했습니다.\n",
      "2017-01-29부터 2017-02-04까지 기사를 32 개 추가했습니다.\n",
      "2017-02-05부터 2017-02-11까지 기사를 33 개 추가했습니다.\n",
      "2017-02-12부터 2017-02-18까지 기사를 21 개 추가했습니다.\n",
      "2017-02-19부터 2017-02-25까지 기사를 44 개 추가했습니다.\n",
      "2017-02-26부터 2017-03-04까지 기사를 84 개 추가했습니다.\n",
      "2017-03-05부터 2017-03-11까지 기사를 33 개 추가했습니다.\n",
      "2017-03-12부터 2017-03-18까지 기사를 79 개 추가했습니다.\n",
      "2017-03-19부터 2017-03-25까지 기사를 115 개 추가했습니다.\n",
      "2017-03-26부터 2017-04-01까지 기사를 41 개 추가했습니다.\n",
      "2017-04-02부터 2017-04-08까지 기사를 49 개 추가했습니다.\n",
      "2017-04-09부터 2017-04-15까지 기사를 38 개 추가했습니다.\n",
      "2017-04-16부터 2017-04-22까지 기사를 36 개 추가했습니다.\n",
      "2017-04-23부터 2017-04-29까지 기사를 42 개 추가했습니다.\n",
      "2017-04-30부터 2017-05-06까지 기사를 19 개 추가했습니다.\n",
      "2017-05-07부터 2017-05-13까지 기사를 51 개 추가했습니다.\n",
      "2017-05-14부터 2017-05-20까지 기사를 37 개 추가했습니다.\n",
      "2017-05-21부터 2017-05-27까지 기사를 41 개 추가했습니다.\n",
      "2017-05-28부터 2017-06-03까지 기사를 39 개 추가했습니다.\n",
      "2017-06-04부터 2017-06-10까지 기사를 43 개 추가했습니다.\n",
      "2017-06-11부터 2017-06-17까지 기사를 63 개 추가했습니다.\n",
      "2017-06-18부터 2017-06-24까지 기사를 45 개 추가했습니다.\n",
      "2017-06-25부터 2017-07-01까지 기사를 45 개 추가했습니다.\n",
      "2017-07-02부터 2017-07-08까지 기사를 38 개 추가했습니다.\n",
      "2017-07-09부터 2017-07-15까지 기사를 113 개 추가했습니다.\n",
      "2017-07-16부터 2017-07-22까지 기사를 53 개 추가했습니다.\n",
      "2017-07-23부터 2017-07-29까지 기사를 160 개 추가했습니다.\n",
      "2017-07-30부터 2017-08-05까지 기사를 48 개 추가했습니다.\n",
      "2017-08-06부터 2017-08-12까지 기사를 15 개 추가했습니다.\n",
      "2017-08-13부터 2017-08-19까지 기사를 53 개 추가했습니다.\n",
      "2017-08-20부터 2017-08-26까지 기사를 57 개 추가했습니다.\n",
      "2017-08-27부터 2017-09-02까지 기사를 32 개 추가했습니다.\n",
      "2017-09-03부터 2017-09-09까지 기사를 59 개 추가했습니다.\n",
      "2017-09-10부터 2017-09-16까지 기사를 93 개 추가했습니다.\n",
      "2017-09-17부터 2017-09-23까지 기사를 36 개 추가했습니다.\n",
      "2017-09-24부터 2017-09-30까지 기사를 50 개 추가했습니다.\n",
      "2017-10-01부터 2017-10-07까지 기사를 6 개 추가했습니다.\n",
      "2017-10-08부터 2017-10-14까지 기사를 44 개 추가했습니다.\n",
      "2017-10-15부터 2017-10-21까지 기사를 74 개 추가했습니다.\n",
      "2017-10-22부터 2017-10-28까지 기사를 201 개 추가했습니다.\n",
      "2017-10-29부터 2017-11-04까지 기사를 529 개 추가했습니다.\n",
      "2017-11-05부터 2017-11-11까지 기사를 140 개 추가했습니다.\n",
      "2017-11-12부터 2017-11-18까지 기사를 37 개 추가했습니다.\n",
      "2017-11-19부터 2017-11-25까지 기사를 41 개 추가했습니다.\n",
      "2017-11-26부터 2017-12-02까지 기사를 51 개 추가했습니다.\n",
      "2017-12-03부터 2017-12-09까지 기사를 120 개 추가했습니다.\n",
      "2017-12-10부터 2017-12-16까지 기사를 49 개 추가했습니다.\n",
      "2017-12-17부터 2017-12-23까지 기사를 57 개 추가했습니다.\n",
      "2017-12-24부터 2017-12-30까지 기사를 22 개 추가했습니다.\n",
      "2017-12-31부터 2018-01-06까지 기사를 30 개 추가했습니다.\n",
      "2018-01-07부터 2018-01-13까지 기사를 49 개 추가했습니다.\n",
      "2018-01-14부터 2018-01-20까지 기사를 37 개 추가했습니다.\n",
      "2018-01-21부터 2018-01-27까지 기사를 72 개 추가했습니다.\n",
      "2018-01-28부터 2018-02-03까지 기사를 140 개 추가했습니다.\n",
      "2018-02-04부터 2018-02-10까지 기사를 25 개 추가했습니다.\n",
      "2018-02-11부터 2018-02-17까지 기사를 28 개 추가했습니다.\n",
      "2018-02-18부터 2018-02-24까지 기사를 135 개 추가했습니다.\n",
      "2018-02-25부터 2018-03-03까지 기사를 34 개 추가했습니다.\n",
      "2018-03-04부터 2018-03-10까지 기사를 48 개 추가했습니다.\n",
      "2018-03-11부터 2018-03-17까지 기사를 49 개 추가했습니다.\n",
      "2018-03-18부터 2018-03-24까지 기사를 40 개 추가했습니다.\n",
      "2018-03-25부터 2018-03-31까지 기사를 39 개 추가했습니다.\n",
      "2018-04-01부터 2018-04-07까지 기사를 61 개 추가했습니다.\n",
      "2018-04-08부터 2018-04-14까지 기사를 36 개 추가했습니다.\n",
      "2018-04-15부터 2018-04-21까지 기사를 127 개 추가했습니다.\n",
      "2018-04-22부터 2018-04-28까지 기사를 433 개 추가했습니다.\n",
      "2018-04-29부터 2018-05-05까지 기사를 149 개 추가했습니다.\n",
      "2018-05-06부터 2018-05-12까지 기사를 306 개 추가했습니다.\n",
      "2018-05-13부터 2018-05-19까지 기사를 92 개 추가했습니다.\n",
      "2018-05-20부터 2018-05-26까지 기사를 44 개 추가했습니다.\n",
      "2018-05-27부터 2018-06-02까지 기사를 32 개 추가했습니다.\n",
      "2018-06-03부터 2018-06-09까지 기사를 27 개 추가했습니다.\n",
      "2018-06-10부터 2018-06-16까지 기사를 42 개 추가했습니다.\n",
      "2018-06-17부터 2018-06-23까지 기사를 50 개 추가했습니다.\n",
      "2018-06-24부터 2018-06-30까지 기사를 47 개 추가했습니다.\n",
      "2018-07-01부터 2018-07-07까지 기사를 51 개 추가했습니다.\n",
      "2018-07-08부터 2018-07-14까지 기사를 46 개 추가했습니다.\n",
      "2018-07-15부터 2018-07-21까지 기사를 42 개 추가했습니다.\n",
      "2018-07-22부터 2018-07-28까지 기사를 73 개 추가했습니다.\n",
      "2018-07-29부터 2018-08-04까지 기사를 39 개 추가했습니다.\n",
      "2018-08-05부터 2018-08-11까지 기사를 29 개 추가했습니다.\n",
      "2018-08-12부터 2018-08-18까지 기사를 45 개 추가했습니다.\n",
      "2018-08-19부터 2018-08-25까지 기사를 38 개 추가했습니다.\n",
      "2018-08-26부터 2018-09-01까지 기사를 36 개 추가했습니다.\n",
      "2018-09-02부터 2018-09-08까지 기사를 38 개 추가했습니다.\n",
      "2018-09-09부터 2018-09-15까지 기사를 150 개 추가했습니다.\n",
      "2018-09-16부터 2018-09-22까지 기사를 56 개 추가했습니다.\n",
      "2018-09-23부터 2018-09-29까지 기사를 18 개 추가했습니다.\n",
      "2018-09-30부터 2018-10-06까지 기사를 35 개 추가했습니다.\n",
      "2018-10-07부터 2018-10-13까지 기사를 273 개 추가했습니다.\n",
      "2018-10-14부터 2018-10-20까지 기사를 131 개 추가했습니다.\n",
      "2018-10-21부터 2018-10-27까지 기사를 252 개 추가했습니다.\n",
      "2018-10-28부터 2018-11-03까지 기사를 57 개 추가했습니다.\n",
      "2018-11-04부터 2018-11-10까지 기사를 51 개 추가했습니다.\n",
      "2018-11-11부터 2018-11-17까지 기사를 33 개 추가했습니다.\n",
      "2018-11-18부터 2018-11-24까지 기사를 52 개 추가했습니다.\n",
      "2018-11-25부터 2018-12-01까지 기사를 45 개 추가했습니다.\n",
      "2018-12-02부터 2018-12-08까지 기사를 17 개 추가했습니다.\n",
      "2018-12-09부터 2018-12-15까지 기사를 19 개 추가했습니다.\n",
      "2018-12-16부터 2018-12-22까지 기사를 34 개 추가했습니다.\n",
      "2018-12-23부터 2018-12-29까지 기사를 32 개 추가했습니다.\n",
      "2018-12-30부터 2019-01-05까지 기사를 19 개 추가했습니다.\n",
      "2019-01-06부터 2019-01-12까지 기사를 130 개 추가했습니다.\n",
      "2019-01-13부터 2019-01-19까지 기사를 54 개 추가했습니다.\n",
      "2019-01-20부터 2019-01-26까지 기사를 23 개 추가했습니다.\n",
      "2019-01-27부터 2019-02-02까지 기사를 189 개 추가했습니다.\n",
      "2019-02-03부터 2019-02-09까지 기사를 36 개 추가했습니다.\n",
      "2019-02-10부터 2019-02-16까지 기사를 34 개 추가했습니다.\n",
      "2019-02-17부터 2019-02-23까지 기사를 40 개 추가했습니다.\n",
      "2019-02-24부터 2019-03-02까지 기사를 25 개 추가했습니다.\n",
      "2019-03-03부터 2019-03-09까지 기사를 33 개 추가했습니다.\n",
      "2019-03-10부터 2019-03-16까지 기사를 36 개 추가했습니다.\n",
      "2019-03-17부터 2019-03-23까지 기사를 12 개 추가했습니다.\n",
      "2019-03-24부터 2019-03-30까지 기사를 26 개 추가했습니다.\n",
      "2019-03-31부터 2019-04-06까지 기사를 29 개 추가했습니다.\n",
      "2019-04-07부터 2019-04-13까지 기사를 39 개 추가했습니다.\n",
      "2019-04-14부터 2019-04-20까지 기사를 60 개 추가했습니다.\n",
      "2019-04-21부터 2019-04-27까지 기사를 181 개 추가했습니다.\n",
      "2019-04-28부터 2019-05-04까지 기사를 44 개 추가했습니다.\n",
      "2019-05-05부터 2019-05-11까지 기사를 17 개 추가했습니다.\n",
      "2019-05-12부터 2019-05-18까지 기사를 45 개 추가했습니다.\n",
      "2019-05-19부터 2019-05-25까지 기사를 41 개 추가했습니다.\n",
      "2019-05-26부터 2019-06-01까지 기사를 38 개 추가했습니다.\n",
      "2019-06-02부터 2019-06-08까지 기사를 32 개 추가했습니다.\n",
      "2019-06-09부터 2019-06-15까지 기사를 34 개 추가했습니다.\n",
      "2019-06-16부터 2019-06-22까지 기사를 204 개 추가했습니다.\n",
      "2019-06-23부터 2019-06-29까지 기사를 50 개 추가했습니다.\n",
      "2019-06-30부터 2019-07-06까지 기사를 49 개 추가했습니다.\n",
      "2019-07-07부터 2019-07-13까지 기사를 39 개 추가했습니다.\n",
      "2019-07-14부터 2019-07-20까지 기사를 26 개 추가했습니다.\n",
      "2019-07-21부터 2019-07-27까지 기사를 183 개 추가했습니다.\n",
      "2019-07-28부터 2019-08-03까지 기사를 51 개 추가했습니다.\n",
      "2019-08-04부터 2019-08-10까지 기사를 33 개 추가했습니다.\n",
      "2019-08-11부터 2019-08-17까지 기사를 50 개 추가했습니다.\n",
      "2019-08-18부터 2019-08-24까지 기사를 37 개 추가했습니다.\n",
      "2019-08-25부터 2019-08-31까지 기사를 38 개 추가했습니다.\n",
      "2019-09-01부터 2019-09-07까지 기사를 41 개 추가했습니다.\n",
      "2019-09-08부터 2019-09-14까지 기사를 40 개 추가했습니다.\n",
      "2019-09-15부터 2019-09-21까지 기사를 44 개 추가했습니다.\n",
      "2019-09-22부터 2019-09-28까지 기사를 58 개 추가했습니다.\n",
      "2019-09-29부터 2019-10-05까지 기사를 49 개 추가했습니다.\n",
      "2019-10-06부터 2019-10-12까지 기사를 30 개 추가했습니다.\n",
      "2019-10-13부터 2019-10-19까지 기사를 39 개 추가했습니다.\n",
      "2019-10-20부터 2019-10-26까지 기사를 56 개 추가했습니다.\n",
      "2019-10-27부터 2019-11-02까지 기사를 59 개 추가했습니다.\n",
      "2019-11-03부터 2019-11-09까지 기사를 45 개 추가했습니다.\n",
      "2019-11-10부터 2019-11-16까지 기사를 63 개 추가했습니다.\n",
      "2019-11-17부터 2019-11-23까지 기사를 169 개 추가했습니다.\n",
      "2019-11-24부터 2019-11-30까지 기사를 22 개 추가했습니다.\n",
      "2019-12-01부터 2019-12-07까지 기사를 38 개 추가했습니다.\n",
      "2019-12-08부터 2019-12-14까지 기사를 30 개 추가했습니다.\n",
      "2019-12-15부터 2019-12-21까지 기사를 35 개 추가했습니다.\n",
      "2019-12-22부터 2019-12-28까지 기사를 26 개 추가했습니다.\n",
      "2019-12-29부터 2020-01-04까지 기사를 26 개 추가했습니다.\n",
      "2020-01-05부터 2020-01-11까지 기사를 33 개 추가했습니다.\n",
      "2020-01-12부터 2020-01-18까지 기사를 24 개 추가했습니다.\n",
      "2020-01-19부터 2020-01-25까지 기사를 45 개 추가했습니다.\n",
      "2020-01-26부터 2020-02-01까지 기사를 131 개 추가했습니다.\n",
      "2020-02-02부터 2020-02-08까지 기사를 56 개 추가했습니다.\n",
      "2020-02-09부터 2020-02-15까지 기사를 27 개 추가했습니다.\n",
      "2020-02-16부터 2020-02-22까지 기사를 35 개 추가했습니다.\n",
      "2020-02-23부터 2020-02-29까지 기사를 47 개 추가했습니다.\n",
      "2020-03-01부터 2020-03-07까지 기사를 42 개 추가했습니다.\n",
      "2020-03-08부터 2020-03-14까지 기사를 54 개 추가했습니다.\n",
      "2020-03-15부터 2020-03-21까지 기사를 39 개 추가했습니다.\n",
      "2020-03-22부터 2020-03-28까지 기사를 34 개 추가했습니다.\n",
      "2020-03-29부터 2020-04-04까지 기사를 32 개 추가했습니다.\n",
      "2020-04-05부터 2020-04-11까지 기사를 118 개 추가했습니다.\n",
      "2020-04-12부터 2020-04-18까지 기사를 54 개 추가했습니다.\n",
      "2020-04-19부터 2020-04-25까지 기사를 237 개 추가했습니다.\n",
      "2020-04-26부터 2020-05-02까지 기사를 51 개 추가했습니다.\n",
      "2020-05-03부터 2020-05-09까지 기사를 32 개 추가했습니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-10부터 2020-05-16까지 기사를 17 개 추가했습니다.\n",
      "2020-05-17부터 2020-05-23까지 기사를 76 개 추가했습니다.\n",
      "2020-05-24부터 2020-05-30까지 기사를 18 개 추가했습니다.\n",
      "2020-05-31부터 2020-06-06까지 기사를 81 개 추가했습니다.\n",
      "2020-06-07부터 2020-06-13까지 기사를 54 개 추가했습니다.\n",
      "2020-06-14부터 2020-06-20까지 기사를 41 개 추가했습니다.\n",
      "2020-06-21부터 2020-06-27까지 기사를 84 개 추가했습니다.\n",
      "2020-06-28부터 2020-07-04까지 기사를 138 개 추가했습니다.\n",
      "2020-07-05부터 2020-07-11까지 기사를 211 개 추가했습니다.\n",
      "2020-07-12부터 2020-07-18까지 기사를 132 개 추가했습니다.\n",
      "2020-07-19부터 2020-07-25까지 기사를 304 개 추가했습니다.\n",
      "2020-07-26부터 2020-08-01까지 기사를 356 개 추가했습니다.\n",
      "2020-08-02부터 2020-08-08까지 기사를 85 개 추가했습니다.\n",
      "2020-08-09부터 2020-08-15까지 기사를 56 개 추가했습니다.\n",
      "2020-08-16부터 2020-08-22까지 기사를 42 개 추가했습니다.\n",
      "2020-08-23부터 2020-08-29까지 기사를 20 개 추가했습니다.\n",
      "2020-08-30부터 2020-09-05까지 기사를 23 개 추가했습니다.\n",
      "2020-09-06부터 2020-09-12까지 기사를 250 개 추가했습니다.\n",
      "2020-09-13부터 2020-09-19까지 기사를 34 개 추가했습니다.\n",
      "2020-09-20부터 2020-09-26까지 기사를 128 개 추가했습니다.\n",
      "2020-09-27부터 2020-10-03까지 기사를 127 개 추가했습니다.\n",
      "2020-10-04부터 2020-10-10까지 기사를 433 개 추가했습니다.\n",
      "2020-10-11부터 2020-10-17까지 기사를 316 개 추가했습니다.\n",
      "2020-10-18부터 2020-10-24까지 기사를 68 개 추가했습니다.\n",
      "2020-10-25부터 2020-10-31까지 기사를 45 개 추가했습니다.\n",
      "2020-11-01부터 2020-11-07까지 기사를 82 개 추가했습니다.\n",
      "2020-11-08부터 2020-11-14까지 기사를 189 개 추가했습니다.\n",
      "2020-11-15부터 2020-11-21까지 기사를 21 개 추가했습니다.\n",
      "2020-11-22부터 2020-11-28까지 기사를 245 개 추가했습니다.\n",
      "2020-11-29부터 2020-12-05까지 기사를 264 개 추가했습니다.\n",
      "2020-12-06부터 2020-12-12까지 기사를 58 개 추가했습니다.\n",
      "2020-12-13부터 2020-12-19까지 기사를 68 개 추가했습니다.\n",
      "2020-12-20부터 2020-12-26까지 기사를 60 개 추가했습니다.\n",
      "2020-12-27부터 2021-01-02까지 기사를 59 개 추가했습니다.\n",
      "2021-01-03부터 2021-01-09까지 기사를 16 개 추가했습니다.\n",
      "2021-01-10부터 2021-01-16까지 기사를 51 개 추가했습니다.\n",
      "2021-01-17부터 2021-01-23까지 기사를 54 개 추가했습니다.\n",
      "2021-01-24부터 2021-01-30까지 기사를 74 개 추가했습니다.\n",
      "2021-01-31부터 2021-02-06까지 기사를 58 개 추가했습니다.\n",
      "2021-02-07부터 2021-02-13까지 기사를 96 개 추가했습니다.\n",
      "2021-02-14부터 2021-02-20까지 기사를 82 개 추가했습니다.\n",
      "2021-02-21부터 2021-02-27까지 기사를 46 개 추가했습니다.\n",
      "2021-02-28부터 2021-03-06까지 기사를 74 개 추가했습니다.\n",
      "2021-03-07부터 2021-03-13까지 기사를 132 개 추가했습니다.\n",
      "2021-03-14부터 2021-03-20까지 기사를 277 개 추가했습니다.\n",
      "2021-03-21부터 2021-03-27까지 기사를 71 개 추가했습니다.\n",
      "2021-03-28부터 2021-04-03까지 기사를 129 개 추가했습니다.\n",
      "2021-04-04부터 2021-04-10까지 기사를 22 개 추가했습니다.\n",
      "2021-04-11부터 2021-04-17까지 기사를 21 개 추가했습니다.\n",
      "2021-04-18부터 2021-04-24까지 기사를 54 개 추가했습니다.\n",
      "2021-04-25부터 2021-05-01까지 기사를 245 개 추가했습니다.\n",
      "2021-05-02부터 2021-05-08까지 기사를 65 개 추가했습니다.\n",
      "2021-05-09부터 2021-05-15까지 기사를 46 개 추가했습니다.\n",
      "2021-05-16부터 2021-05-22까지 기사를 42 개 추가했습니다.\n",
      "2021-05-23부터 2021-05-29까지 기사를 78 개 추가했습니다.\n",
      "2021-05-30부터 2021-06-05까지 기사를 66 개 추가했습니다.\n",
      "2021-06-06부터 2021-06-12까지 기사를 232 개 추가했습니다.\n",
      "2021-06-13부터 2021-06-19까지 기사를 197 개 추가했습니다.\n",
      "2021-06-20부터 2021-06-26까지 기사를 395 개 추가했습니다.\n",
      "2021-06-27부터 2021-07-03까지 기사를 69 개 추가했습니다.\n",
      "2021-07-04부터 2021-07-10까지 기사를 256 개 추가했습니다.\n",
      "2021-07-11부터 2021-07-17까지 기사를 225 개 추가했습니다.\n",
      "2021-07-18부터 2021-07-24까지 기사를 225 개 추가했습니다.\n",
      "2021-07-25부터 2021-07-31까지 기사를 67 개 추가했습니다.\n",
      "2021-08-01부터 2021-08-07까지 기사를 58 개 추가했습니다.\n",
      "2021-08-08부터 2021-08-14까지 기사를 70 개 추가했습니다.\n",
      "2021-08-15부터 2021-08-21까지 기사를 188 개 추가했습니다.\n",
      "2021-08-22부터 2021-08-28까지 기사를 51 개 추가했습니다.\n",
      "2021-08-29부터 2021-09-04까지 기사를 131 개 추가했습니다.\n",
      "2021-09-05부터 2021-09-11까지 기사를 350 개 추가했습니다.\n",
      "2021-09-12부터 2021-09-18까지 기사를 276 개 추가했습니다.\n",
      "2021-09-19부터 2021-09-25까지 기사를 54 개 추가했습니다.\n",
      "2021-09-26부터 2021-10-02까지 기사를 178 개 추가했습니다.\n",
      "2021-10-03부터 2021-10-09까지 기사를 186 개 추가했습니다.\n",
      "2021-10-10부터 2021-10-16까지 기사를 25 개 추가했습니다.\n",
      "2021-10-17부터 2021-10-23까지 기사를 121 개 추가했습니다.\n",
      "2021-10-24부터 2021-10-30까지 기사를 59 개 추가했습니다.\n",
      "2021-10-31부터 2021-11-06까지 기사를 57 개 추가했습니다.\n",
      "2021-11-07부터 2021-11-13까지 기사를 99 개 추가했습니다.\n",
      "2021-11-14부터 2021-11-20까지 기사를 200 개 추가했습니다.\n",
      "2021-11-21부터 2021-11-27까지 기사를 135 개 추가했습니다.\n",
      "2021-11-28부터 2021-12-04까지 기사를 62 개 추가했습니다.\n",
      "2021-12-05부터 2021-12-11까지 기사를 130 개 추가했습니다.\n",
      "2021-12-12부터 2021-12-18까지 기사를 37 개 추가했습니다.\n",
      "2021-12-19부터 2021-12-25까지 기사를 53 개 추가했습니다.\n",
      "2021-12-26부터 2022-01-01까지 기사를 94 개 추가했습니다.\n",
      "2022-01-02부터 2022-01-08까지 기사를 113 개 추가했습니다.\n",
      "2022-01-09부터 2022-01-15까지 기사를 140 개 추가했습니다.\n",
      "2022-01-16부터 2022-01-22까지 기사를 131 개 추가했습니다.\n",
      "2022-01-23부터 2022-01-29까지 기사를 272 개 추가했습니다.\n",
      "2022-01-30부터 2022-02-05까지 기사를 122 개 추가했습니다.\n",
      "2022-02-06부터 2022-02-12까지 기사를 116 개 추가했습니다.\n",
      "2022-02-13부터 2022-02-19까지 기사를 29 개 추가했습니다.\n",
      "2022-02-20부터 2022-02-26까지 기사를 59 개 추가했습니다.\n",
      "2022-02-27부터 2022-03-05까지 기사를 118 개 추가했습니다.\n",
      "2022-03-06부터 2022-03-12까지 기사를 210 개 추가했습니다.\n",
      "2022-03-13부터 2022-03-19까지 기사를 236 개 추가했습니다.\n",
      "2022-03-20부터 2022-03-26까지 기사를 56 개 추가했습니다.\n",
      "2022-03-27부터 2022-03-31까지 기사를 44 개 추가했습니다.\n",
      "\n",
      "\n",
      "현재 폴더에 네이버_170101~220331 테스트 데이터.xlsx 이름으로 저장했습니다.\n",
      "Wall time: 3h 14min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52e8eae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23175"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(df[\"category\"]==\"경제\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c29b53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23175"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41c01c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 23175 entries, 0 to 23174\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     23175 non-null  object\n",
      " 1   category  23175 non-null  object\n",
      " 2   date      23175 non-null  object\n",
      " 3   paper     23175 non-null  object\n",
      " 4   url       23175 non-null  object\n",
      " 5   content   23175 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15f08538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IT      11992\n",
       "경제       7889\n",
       "사회       1518\n",
       "정치       1016\n",
       "생활        568\n",
       "오피니언      115\n",
       "세계         77\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96e94d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "연합뉴스       1737\n",
       "뉴시스        1724\n",
       "이데일리       1574\n",
       "뉴스1        1522\n",
       "한국경제       1182\n",
       "           ... \n",
       "앳스타일          2\n",
       "코메디닷컴         1\n",
       "뉴스타파          1\n",
       "kbc광주방송       1\n",
       "국제신문          1\n",
       "Name: paper, Length: 81, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"paper\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791c4cd",
   "metadata": {},
   "source": [
    "#### 기사가 집중된 특정 기간에 대한 확인이 필요하니 기간+기사갯수가 있는 df가 따로 있으면 좋을듯함\n",
    "#### 주간단위를 끊어서 가져가는 것도 괜찮을듯\n",
    "#### 월별기사를 따로 정리하는 함수를 삽입하는 것도 괜찮을듯..\n",
    "#### 주가데이터 상승,하락이 모두 만들어지면 연동해서 EDA하는 것도 괜찮을듯.. 날짜가 주요 골자가 되겠지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ae8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
